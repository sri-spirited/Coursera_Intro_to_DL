1. Choose correct statements about convolutional layer:

	- Convolutional layer works the same way for every input patch TRUE
	- Convolutional layer provides translation invariance FALSE: IT IS TRANSLATION EQUIVARIANT 
	- Convolutional layer doesn't need a bias term FALSE 
	- Convolutional layer is a special case of a fully-connected layer TRUE 

2.  Choose correct statements about pooling layer:
	- Pooling layer can reduce spatial dimensions (width and height of the input volume) TRUE 
	- Pooling layer provides translation invariance TRUE
	- Pooling layer reduces the number of convolutional filters FALSE 
	- Pooling layer is strictly differentiable FALSE 


3.Back-propagation for convolutional layer first calculates the gradients as if the kernel parameters were not shared and then...
	- Takes a maximum gradient for each shared parameter
	- Takes a minimum gradient for each shared parameter
	- Takes a sum of gradients for each shared parameter TRUE 
	- Takes a mean of the gradients for each shared parameter

4. Suppose you have a 10x10x3 colour image input and you want to stack two convolutional layers with kernel size 3x3 with 10 and 20 filters respectively. How many parameters do you have to train for these two layers? Don't forget bias terms!
	- 2100 
	How: 
	Weights in 1st conv layer: (3 width * 3 height * 3 channels) + 1 bias) * 10 feature maps = (3*3*3 + 1)*10 = 280
	Weights in 2nd conv layer: (3 width * 3 height * 10 channels from conv layer 1 + 1 bias)* 20 feat maps = (3*3*10 + 1)*20 = 1820
	Total = 280 + 1820 = 2100 
 
5.What receptive field do we have after stacking nn convolutional layers with kernel size k \times kk×k and stride 1? Layers numeration starts with 1. The resulting receptive field will be a square, input its side as an answer.

	- 1+n*(k-1) 
	Refer http://people.whitman.edu/~hundledr/courses/M350/cnn-cheat.pdf
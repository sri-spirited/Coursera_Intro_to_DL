{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1.__ Choose the correct statements about MLP\n",
    "\n",
    "\n",
    "* MLP with a linear activation function is better than a linear model __false__\n",
    "* The first hidden layer contains predictions for your task __false__\n",
    "* We can train MLP with SGD __true__\n",
    "* MLP can have only 1 hidden layer __false__ \n",
    "* A hidden layer of MLP automatically learns new helpful features for the task __true__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------\n",
    "\n",
    "__2.__ Apply a chain rule to calculate ${\\partial a} \\over {\\partial x}$ where $a(x, y) = sin(xy) \\cdot e^x$\n",
    ".Here is an example of the syntax: sin(x*y)*exp(x), more info here\n",
    "\n",
    "\n",
    "\n",
    "__Ans.__  y*cos(x*y)*exp(x)+exp(x)*sin(x*y)\n",
    "         \n",
    "--------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__3.__Choose the correct statements about backpropagation\n",
    "CORRECT \n",
    "* It is done in one pass __false__ \n",
    "* It is the way to train modern neural networks __true__\n",
    "* You can use non-differentiable loss to train your MLP __false__\n",
    "* It is an efficient way to apply a chain rule __true__ \n",
    "--------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__4.__ What is the time complexity of backpropagation algorithm w.r.t. number of edges NN in the computational graph?\n",
    "\n",
    "$O(N)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1.__ Choose the correct statements about MLP implementation: \n",
    "* You can write both passes of a dense layer with NumPy and make it quick even in Python __True__\n",
    "* You shouldn't prefer matrix operations when working with GPU __False__\n",
    "* A backward pass of a dense layer needs a 4-d tensor derivative __False__\n",
    "* A forward pass of a dense layer can be done with matrix product __True__\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "\n",
    "__2__ How many dimensions will a derivative of a 3-d tensor by a 4-d tensor have?\n",
    "\n",
    "__7__\n",
    "--------------------------------------------------------------------------------\n",
    "\n",
    "__3__  Let's play around with matrix derivatives!\n",
    "\n",
    "A trace $Tr(X)$ of a matrix $X$ is a sum of its diagonal elements.\n",
    "\n",
    "For example: $Tr\\begin{pmatrix} 1 & 3 \\\\ 3 & 1 \\end{pmatrix}=1+1=2$. Note that trace is a scalar!\n",
    "\n",
    "\n",
    "Let's find a matrix notation for ${\\partial Tr(X^2) \\over \\partial X}$ for matrix $X=\\begin{pmatrix} x_{1,1} & x_{1,2} \\\\ x_{2,1} & x_{2,2} \\end{pmatrix}$ where  where $X^2$ is a matrix product $X \\cdot X$. Please do this element-wise and figure out a matrix notation for it:\n",
    "* $2Tr(X^T)$\n",
    "* $2X^T$\n",
    "* ${X^T}X$\n",
    "* $Tr(2X)$\n",
    "* $2X$ __This__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
